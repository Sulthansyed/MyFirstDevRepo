{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the numpy library, it is the only library will be used across the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost error method\n",
    "1. calculate the hypothesis function value y = mx + b\n",
    "2. Calculate the difference between the predicted and the actual output\n",
    "3. Square the error difference to keep the error minimized\n",
    "4. Do it for all the input x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costerror(b,m,points):\n",
    "    totalerror = 0\n",
    "    #hypothesis function prediction = mx+b\n",
    "    #totalerror is actual-prediction to minimize the error and avoid negative we are squaring up\n",
    "    #to average the value we will divide it by total no o examples\n",
    "    for i in range(0,len(points)):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        prediction = (m*x)+b\n",
    "        totalerror += (y-prediction)**2\n",
    "    return totalerror/float(len(points))   \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the importance performance measure step is to calculate the gradient of the squared error to get at which point of constant the slope converge at 0(This is the point where the error difference will be very low between the actual and predicted value).This determine the best fit line across the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steep_gradient(b_current,m_current,points,learningrate):\n",
    "    b_grad=0\n",
    "    m_grad=0\n",
    "    N=float(len(points))\n",
    "    for i in range(0,len(points)):\n",
    "        x=points[i,0]\n",
    "        y=points[i,1]\n",
    "        prediction=(m_current*x) + b_current\n",
    "        #Gradient is calculated by doing the partial derivative on squared error function\n",
    "        b_grad += -(2/N) * (y-prediction)\n",
    "        m_grad += -(2/N) * x * (y-prediction)\n",
    "    #learningrate is a hyperparameter which decide at what rate our slope points will converge\n",
    "    #new slope points are kept negative as we are going down the slope\n",
    "    new_b = b_current-(learningrate * b_grad)\n",
    "    new_m = m_current-(learningrate  *m_grad)\n",
    "    return[new_b,new_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientdescentrunner(points,starting_b,starting_m,learningrate,no_iterations):\n",
    "    #intializing constants with 0,0\n",
    "    b=starting_b\n",
    "    m=starting_m\n",
    "    #over the no of iterations optimum constant will be returnted\n",
    "    for i in range(no_iterations):\n",
    "        b,m=steep_gradient(b,m,array(points),learningrate)\n",
    "    return[b,m]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialising run method and passing all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    #loading the dataset\n",
    "    points = genfromtxt('data.csv',delimiter=',')\n",
    "    #loading the hyperparameters i.e constant,learningrate\n",
    "    learningrate = 0.0001\n",
    "    no_iterations=1000\n",
    "    intial_b = 0\n",
    "    intial_m = 0\n",
    "    #calling the costerror and gradient function\n",
    "    print 'Start of gradient at b={0},m={1},error={2}'.format(intial_b,intial_m,costerror(intial_b,intial_m,points))\n",
    "    print 'running===>'\n",
    "    [b,m]=gradientdescentrunner(points,intial_b,intial_m,learningrate,no_iterations)\n",
    "    print 'After {0} iterations,b={1},m={2},error={3}'.format(no_iterations,b,m,costerror(b,m,points))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of gradient at b=0,m=0,error=5565.10783448\n",
      "running===>\n",
      "After 1000 iterations,b=0.0889365199374,m=1.47774408519,error=112.614810116\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:siraj_linearregression]",
   "language": "python",
   "name": "conda-env-siraj_linearregression-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
